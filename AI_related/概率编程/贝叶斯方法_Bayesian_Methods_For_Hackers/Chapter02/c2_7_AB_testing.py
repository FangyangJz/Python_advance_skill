# !/usr/bin/env python3
# -*- coding:utf-8 -*-

# @Datetime : 2020/1/3 下午10:24
# @Author   : Fangyang
# @Software : PyCharm


import pymc3 as pm
import torch
import pyro.distributions as pyro_dist
import torch.distributions as torch_dist
import pyro
import numpy as np
import theano.tensor as tt
import matplotlib.pyplot as plt
import scipy.stats as stats
import pandas as pd
from pyro.infer import MCMC, NUTS, HMC


def pymc3_mh_infer(obs):
    # The parameters are the bounds of the Uniform.
    with pm.Model() as model:
        p = pm.Uniform('p', lower=0, upper=1)

    # include the observations, which are Bernoulli
    with model:
        obs = pm.Bernoulli("obs", p, observed=obs)
        # To be explained in chapter 3
        step = pm.Metropolis()
        trace = pm.sample(18000, step=step)
        burned_trace = trace[1000:]

    return burned_trace


def pyro_model(occurrences):
    # a = pyro.sample("a", dist.Normal(0., 10.))
    # b_a = pyro.sample("bA", dist.Normal(0., 1.))
    # b_r = pyro.sample("bR", dist.Normal(0., 1.))
    # b_ar = pyro.sample("bAR", dist.Normal(0., 1.))
    # sigma = pyro.sample("sigma", dist.Uniform(0., 10.))
    # mean = a + b_a * is_cont_africa + b_r * ruggedness + b_ar * is_cont_africa * ruggedness
    # with pyro.plate("data", len(occurrences)):
    #     pyro.sample("obs", pyro_dist.Bernoulli(p), obs=occurrences)

    p_true = 0.05  # remember, this is unknown.
    N = 1500
    occurrences = stats.bernoulli.rvs(p_true, size=N)
    occurrences = torch.from_numpy(occurrences).double()
    # 写torch_dist会报错
    p = pyro.sample("p", pyro_dist.Uniform(low=0.0, high=1.0))

    # obs = occurrences.view(-1).numpy()
    # plt.plot(np.arange(len(obs)), obs)
    # plt.show()
    # print(occurrences.max())
    y = pyro.sample("y", pyro_dist.Bernoulli(p), obs=occurrences)
    return y


def pyro_NUTS_infer(model):
    # No-U-Turn Sampler内核，
    # 它提供了一种高效，便捷的方式来运行 Hamiltonian Monte Carlo

    nuts_kernel = NUTS(model, adapt_step_size=True)

    # num_samples（int）–需要生成的样本数，不包括在预热阶段丢弃的样本。
    # warmup_steps（int）-预热迭代次数。在预热阶段生成的样本将被丢弃。如果未提供，则默认值为num_samples的一半。
    # num_chains（int）–要并行运行的MCMC链数。根据num_chains是1还是大于1，此类在内部调度到_UnarySampler或_MultiSampler。
    # initial_params（dict）–包含无约束空间中的初始张量的dict，用于启动马尔可夫链。前导维的大小必须与num_chains的大小匹配。如果未指定，则参数值将从先前采样。
    # hook_fn –可调用的Python，以（内核，示例，阶段，i） 作为参数。阶段是样本或预热，我指的是给定阶段的第i个样本。这可用于实现其他日志记录，或更一般地，每个生成的样本运行任意代码。
    # mp_context（str）– num_chains> 1时要使用的多处理上下文。仅适用于Python 3.5及更高版本。将mp_context =“ spawn”用于CUDA。
    # disable_progbar（bool）–禁用进度条和诊断更新。
    # disable_validation（bool）–禁用分发验证检查。默认情况下将其禁用，因为不同的过渡会导致异常。切换到True以进行调试。
    # transforms（dict）–字典，用于指定对不受限制空间的约束的示例站点的变换。

    mcmc = MCMC(nuts_kernel, num_samples=18000)
    # No-U-Turn Sampler kernel, which provides an efficient and convenient way to run Hamiltonian Monte Carlo.
    # The number of steps taken by the integrator is dynamically adjusted on each call to sample
    # to ensure an optimal length for the Hamiltonian trajectory [1].
    # As such, the samples generated will typically have lower autocorrelation than those generated by the HMC kernel.
    # Optionally, the NUTS kernel also provides the ability to adapt step size during the warmup phase.
    mcmc.run()

    samples = {k: v.detach().cpu().numpy() for k, v in mcmc.get_samples().items()}
    return samples


def pyro_HMC_infer(model):
    # No-U-Turn Sampler内核，
    # 它提供了一种高效，便捷的方式来运行 Hamiltonian Monte Carlo
    hmc_kernel = HMC(model, step_size=1, num_steps=40)

    # model –包含Pyro原语的Python可调用对象。
    # potential_fn –可调用的Python可计算的带输入势能是实际支持参数的决定。
    # step_size（float）–确定在使用汉密尔顿动力学计算轨迹时，顶点积分器采取的单个步长的大小。如果未指定，它将设置为1。
    # trajectory_length（float）– MCMC轨迹的长度。如果未指定，它将设置为step_size x num_steps。如果未指定num_steps,它将设置为2π。
    # num_steps（int）–模拟汉密尔顿动力学的离散步数。轨迹末尾的状态将作为建议返回。该值始终等于 。int(trajectory_length / step_size)
    # Adapt_step_size（bool）–一个标志，用于确定是否要在预热阶段使用Dual Averaging方案来调整step_size。
    # Adapt_mass_matrix（bool）–一个标志，用于决定是否要在预热阶段使用Welford方案来适配质量矩阵。
    # full_mass（bool）–一个标志，用于确定质量矩阵是密集的还是对角线的。
    # transforms（dict）–可选字典，用于指定对不受约束空间的约束支持的示例站点的变换。转换应该是可逆的，并实现log_abs_det_jacobian。如果未指定，并且模型具有受支持的站点，则将应用自动转换，如中所述 torch.distributions.constraint_registry。
    # max_plate_nesting（int）–嵌套pyro.plate()上下文的最大数量上的可选界限 。如果模型包含可以并行枚举的离散样本位点，则需要这样做。
    # jit_compile（bool）–可选参数，指示是否使用PyTorch JIT跟踪日志密度计算，并在积分器中使用此优化的可执行跟踪。
    # jit_options（dict）–词典包含torch.jit.trace()函数的可选参数 。
    # ignore_jit_warnings（bool）-标记为在时忽略来自JIT跟踪器的警告jit_compile=True。默认值为False。
    # target_accept_prob（float）–增大此值将导致较小的步长，因此采样将更慢且更可靠。默认为0.8。

    mcmc = MCMC(hmc_kernel, num_samples=18000)
    # No-U-Turn Sampler kernel, which provides an efficient and convenient way to run Hamiltonian Monte Carlo.
    # The number of steps taken by the integrator is dynamically adjusted on each call to sample
    # to ensure an optimal length for the Hamiltonian trajectory [1].
    # As such, the samples generated will typically have lower autocorrelation than those generated by the HMC kernel.
    # Optionally, the NUTS kernel also provides the ability to adapt step size during the warmup phase.
    data = torch.randint(0, 2, (20000,))
    mcmc.run(data)
    samples = {k: v.detach().cpu().numpy() for k, v in mcmc.get_samples().items()}
    return samples


if __name__ == '__main__':
    # set constants
    p_true = 0.05  # remember, this is unknown.
    N = 1500

    # sample N Bernoulli random variables from Ber(0.05).
    # each random variable has a 0.05 chance of being a 1.
    # this is the data-generation step
    occurrences = stats.bernoulli.rvs(p_true, size=N)

    print(occurrences)  # Remember: Python treats True == 1, and False == 0
    print(np.sum(occurrences))

    # Occurrences.mean is equal to n/N.
    print("What is the observed frequency in Group A? %.4f" % np.mean(occurrences))
    print("Does this equal the true frequency? %s" % (np.mean(occurrences) == p_true))

    burned_trace = pymc3_mh_infer(occurrences)

    # samples = pyro_HMC_infer(pyro_model)
    # print(samples)

    plt.title("Posterior distribution of $p_A$, the true effectiveness of site A")
    plt.vlines(p_true, 0, 90, linestyle="--", label="true $p_A$ (unknown)")
    plt.hist(burned_trace["p"], bins=25, histtype="stepfilled", normed=True)
    plt.legend()
    plt.show()
    print(1)
